{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Neural Network",
   "id": "46eb010b9558ab77"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> Back Propagation Implementation - December 2024\n",
    ">\n",
    "> NEC First Assignment - Universitat Rovira i Virgili\n",
    ">\n",
    "> *Andrea Pujals Bocero*"
   ],
   "id": "a409f48d64643674"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Workflow\n",
    "1. Initialization\n",
    "2. Feed-forward propagation\n",
    "3. Error back-propagation\n",
    "4. Update of weights and thresholds\n",
    "5. Training loop\n",
    "6. Evaluation and visualization"
   ],
   "id": "dbb541315ca82e9c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T11:19:20.091163Z",
     "start_time": "2024-12-04T11:19:18.291737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "c0bc2a4329e913e6",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. Initialization\n",
    "Define parameters in the class constructor. Random initialization of weights to break symmetry and scaled to 0.01. Zero initialization of activations and deltas.\n"
   ],
   "id": "e8d27a359038ffe5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T11:21:22.560065Z",
     "start_time": "2024-12-04T11:21:22.539817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self, layers, epochs, learning_rate, momentum, fact, val_split):\n",
    "        self.L = len(layers)  # Number of layers\n",
    "        self.n = layers       # Number of units in each layer\n",
    "        self.epochs = epochs  # Number of training epochs\n",
    "        self.lr = learning_rate  # Learning rate\n",
    "        self.momentum = momentum  # Momentum term\n",
    "        self.fact = fact  # Activation function\n",
    "        self.val_split = val_split  # Validation set percentage\n",
    "        \n",
    "        # Initialize activations, weights, thresholds, and other variables\n",
    "        self.h = [np.zeros(n) for n in layers]  # Fields\n",
    "        self.xi = [np.zeros(n) for n in layers]  # Activations\n",
    "        self.w = [None] + [np.random.randn(layers[i], layers[i-1]) for i in range(1, self.L)]\n",
    "        self.theta = [None] + [np.random.randn(layers[i]) for i in range(1, self.L)]\n",
    "        #self.w = [None] + [np.random.randn(layers[i], layers[i-1]) * 0.01 for i in range(1, self.L)]  # Weights\n",
    "        #self.theta = [np.zeros(n) for n in layers]  # Thresholds\n",
    "        self.delta = [np.zeros(n) for n in layers]  # Propagated errors\n",
    "        self.d_w = [None] + [np.zeros_like(w) for w in self.w[1:]]  # Weight updates\n",
    "        self.d_theta = [np.zeros(n) for n in layers]  # Threshold updates\n",
    "        self.d_w_prev = [None] + [np.zeros_like(w) for w in self.w[1:]]  # Previous weight updates\n",
    "        self.d_theta_prev = [np.zeros(n) for n in layers]  # Previous threshold updates\n",
    "        \n",
    "        # Loss tracking\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []"
   ],
   "id": "c92027f17f6d01d0",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Activation funciton method: This method applies a chosen activation function to the input h, determining the output of neurons in a layer.\n",
    "Activation functions introduce non-linearity, enabling the network to model complex relationships.\n",
    "\n",
    "Sigmoid function:\n",
    "\n",
    "$ g(h) = \\frac{1}{1 + e^{-h}} $\n",
    "\n",
    "Relu function:\n",
    "\n",
    "$ g(h) = \\max(0, h) $\n",
    "\n",
    "Tanh function:\n",
    "\n",
    "$ g(h) = \\tanh(h) $\n",
    "\n",
    "Linear function:\n",
    "\n",
    "$ g(h) = h $"
   ],
   "id": "63d6f9a5959df4a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T11:21:50.004391Z",
     "start_time": "2024-12-04T11:21:50.001138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "    def activation(self, h):\n",
    "        # Compute the activation function\n",
    "        if self.fact == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-h))\n",
    "        elif self.fact == 'relu':\n",
    "            return np.maximum(0, h)\n",
    "        elif self.fact == 'tanh':\n",
    "            return np.tanh(h)\n",
    "        elif self.fact == 'linear':\n",
    "            return h"
   ],
   "id": "929fd8d193bdf13d",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Then, the derivative is used during back-propagation to compute the gradient of the loss function with respect to the weights:\n",
    "\n",
    "Derivative of sigmoid function:\n",
    "\n",
    "$ g'(h) = g(h) \\cdot (1 - g(h)) $\n",
    "\n",
    "Derivative of relu function:\n",
    "\n",
    "$ g'(h) =\n",
    "\\begin{cases} \n",
    "1 & \\text{if } h > 0 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "Derivative of tanh function:\n",
    "\n",
    "$ g'(h) = 1 - \\tanh^2(h) $\n",
    "\n",
    "Derivative of linear function:\n",
    "\n",
    "$ g'(h) = 1 $\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "118740ec3de03691"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T11:21:52.347760Z",
     "start_time": "2024-12-04T11:21:52.342962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "    def activation_derivative(self, h):\n",
    "        # Compute the derivative of the activation function\n",
    "        if self.fact == 'sigmoid':\n",
    "            act = 1 / (1 + np.exp(-h))\n",
    "            return act * (1 - act)\n",
    "        elif self.fact == 'relu':\n",
    "            return np.where(h > 0, 1, 0)\n",
    "        elif self.fact == 'tanh':\n",
    "            return 1 - np.tanh(h) ** 2\n",
    "        elif self.fact == 'linear':\n",
    "            return np.ones_like(h)"
   ],
   "id": "3db0f57f4dbb2224",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Feed-forward propagation\n",
    "The pre-activation field for each layer is computed as:\n",
    "\n",
    "$ \\xi^{(\\ell)}_i = g(h^{(\\ell)}_i) $\n",
    "\n",
    "The activation for each unit is computed using an activation function ùëî:\n",
    "\n",
    "$h^{(\\ell)}_i = \\sum_j w^{(\\ell)}_{ij} \\cdot \\xi^{(\\ell-1)}_j - \\theta^{(\\ell)}_i$"
   ],
   "id": "ece57bdb2d9b23c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T11:21:55.473537Z",
     "start_time": "2024-12-04T11:21:55.452514Z"
    }
   },
   "cell_type": "code",
   "source": [
    "    def forward(self, X):\n",
    "      # Compute forward propagation\n",
    "\n",
    "      self.xi[0] = X  # Input layer activations\n",
    "      for l in range(1, self.L):\n",
    "        self.h[l] = np.dot(self.w[l], self.xi[l - 1]) - self.theta[l]\n",
    "        self.xi[l] = self.activation(self.h[l])\n",
    "      return self.xi[-1]  # Return output layer activations"
   ],
   "id": "a7f1233ea6db1e2a",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Error back-propagation\n",
    "The error for the output layer L is:\n",
    "\n",
    "$ \\Delta^{(L)}_i = g'(h^{(L)}_i) \\cdot (o_i{(x)} - z_i) $\n",
    "\n",
    "For each hidden layer ‚Ñì, the error is propagated backward:\n",
    "\n",
    "$ \\Delta^{(\\ell-1)}_j = g'(h^{(\\ell-1)}_j) \\cdot \\sum_i \\Delta^{(\\ell)}_i \\cdot w^{(\\ell)}_{ij} $\n",
    "\n",
    "\n"
   ],
   "id": "ee08225bb08b476e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T11:21:57.460185Z",
     "start_time": "2024-12-04T11:21:57.452142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "    def backward(self, y_true):\n",
    "      # Compute backward propagation\n",
    "\n",
    "      # Compute delta for the output layer\n",
    "      self.delta[-1] = (self.xi[-1] - y_true) * self.activation_derivative(self.h[-1])\n",
    "      # Propagate errors backward\n",
    "      for l in range(self.L - 2, 0, -1):\n",
    "        self.delta[l] = np.dot(self.w[l + 1].T, self.delta[l + 1]) * self.activation_derivative(self.h[l])"
   ],
   "id": "eb085c1da5b7a59c",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. Update of weights and thresholds\n",
    "The weights are updated using the delta rule with momentum:\n",
    "\n",
    "$ \\delta w^{(\\ell)}_{ij} = -\\eta \\cdot \\Delta^{(\\ell)}_i \\cdot \\xi^{(\\ell-1)}_j + \\alpha \\cdot \\delta w^{(\\ell)}_{ij, \\text{prev}} $\n",
    "\n",
    "$ w^{(\\ell)}_{ij} \\to w^{(\\ell)}_{ij} + \\delta w^{(\\ell)}_{ij} $\n",
    "\n",
    "Thresholds are updated similarly:\n",
    "\n",
    "$ \\delta \\theta^{(\\ell)}_i = \\eta \\cdot \\Delta^{(\\ell)}_i + \\alpha \\cdot \\delta \\theta^{(\\ell)}_i(\\text{prev}) $\n",
    "\n",
    "$ \\theta^{(\\ell)}_i \\to \\theta^{(\\ell)}_i + \\delta \\theta^{(\\ell)}_i $\n",
    "\n"
   ],
   "id": "a1d6118df87622b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T11:21:59.619301Z",
     "start_time": "2024-12-04T11:21:59.612911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "    def update_weights_thresholds(self):\n",
    "        # Update weights and thresholds using momentum\n",
    "        for l in range(1, self.L):\n",
    "            self.d_w[l] = -self.lr * np.outer(self.delta[l], self.xi[l - 1]) + self.momentum * self.d_w_prev[l]\n",
    "            self.w[l] += self.d_w[l]\n",
    "            self.d_w_prev[l] = self.d_w[l]\n",
    "\n",
    "            self.d_theta[l] = self.lr * self.delta[l] + self.momentum * self.d_theta_prev[l]\n",
    "            self.theta[l] += self.d_theta[l]\n",
    "            self.d_theta_prev[l] = self.d_theta[l]"
   ],
   "id": "ce0cf8e9a8905dc1",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5. Training loop\n",
    "Shuffle data, loop through epochs, minimize quadratic error:\n",
    "\n",
    "$ E[o] = \\frac{1}{2} \\sum_{\\mu=1}^p \\sum_{i=1}^m (o_i(x^{\\mu}) - z_i^{\\mu})^2 $\n"
   ],
   "id": "86111a6e7a68f0e0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T11:22:02.142349Z",
     "start_time": "2024-12-04T11:22:02.130719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "    def fit(self, X, y):\n",
    "        # Train the neural network\n",
    "\n",
    "        # Split data into training and validation sets\n",
    "        n_train = int((1 - self.val_split) * len(X))\n",
    "        X_train, X_val = X[:n_train], X[n_train:]\n",
    "        y_train, y_val = y[:n_train], y[n_train:]\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            # Shuffle training data\n",
    "            indices = np.random.permutation(len(X_train))\n",
    "            X_train, y_train = X_train[indices], y_train[indices]\n",
    "\n",
    "            # Train on each sample\n",
    "            for i in range(len(X_train)):\n",
    "                self.forward(X_train[i])\n",
    "                self.backward(y_train[i])\n",
    "                self.update_weights_thresholds()\n",
    "\n",
    "            # Compute losses\n",
    "            train_loss = np.mean((self.predict(X_train) - y_train) ** 2)\n",
    "            val_loss = np.mean((self.predict(X_val) - y_val) ** 2)\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.val_losses.append(val_loss)\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{self.epochs}, Train Loss: {train_loss}, Val Loss: {val_loss}\")"
   ],
   "id": "ad2ab68c6db4f401",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6. Evaluation and visualization\n",
    "Evaluating the model‚Äôs performance on unseen data (test sets). The predict method corresponds to the output layer‚Äôs activations during forward propagation:\n",
    "\n",
    "$ o(x) = \\xi^{(L)} $\n"
   ],
   "id": "5c857dead568d224"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T11:22:07.294066Z",
     "start_time": "2024-12-04T11:22:07.285820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "    def predict(self, X):\n",
    "        # Generate predictions\n",
    "        predictions = []\n",
    "        for sample in X:\n",
    "            predictions.append(self.forward(sample))\n",
    "        return np.array(predictions)"
   ],
   "id": "82f54de8fea4ec71",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Error visualization. Training error should decrease.",
   "id": "24be843f22c36053"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "    def loss_epochs(self):\n",
    "        # Return the evolution of the training loss and the validation loss\n",
    "        return self.train_losses, self.val_losses"
   ],
   "id": "526ad46357790a10"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-04T11:22:09.913948Z",
     "start_time": "2024-12-04T11:22:09.908388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "    def plot_errors(self):\n",
    "        # Plot training and validation losses\n",
    "        epochs = np.arange(1, len(self.train_losses) + 1)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(epochs, self.train_losses, label=\"Training Loss\", marker='o')\n",
    "        plt.plot(epochs, self.val_losses, label=\"Validation Loss\", marker='o')\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Mean Squared Error (MSE)\") # Loss\n",
    "        plt.legend()\n",
    "        plt.title(\"Training and Validation Loss Over Epochs\")\n",
    "        plt.grid()\n",
    "        plt.show()"
   ],
   "id": "aee896acd307a8a5",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Example of usage",
   "id": "73b2032dcac4d785"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Define network architecture and parameters\n",
    "    layers = [4, 9, 5, 1]\n",
    "    epochs = 100\n",
    "    learning_rate = 0.01\n",
    "    momentum = 0.9\n",
    "    activation_function = 'sigmoid'  # Options: 'sigmoid', 'relu', 'tanh', 'linear'\n",
    "    validation_split = 0.2  # 20% of data used for test\n",
    "\n",
    "    nn = NeuralNet(layers, epochs, learning_rate, momentum, activation_function, validation_split)\n",
    "\n",
    "    # Example synthetic dataset\n",
    "    X = np.random.rand(100, 4)  # 100 samples, 4 features\n",
    "    y = np.random.rand(100, 1)  # 100 target values\n",
    "\n",
    "    nn.fit(X, y)\n",
    "    predictions = nn.predict(X)\n",
    "\n",
    "    # Visualize training and validation errors\n",
    "    nn.plot_errors()"
   ],
   "id": "46cd5859541212f8",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
